PrivyTune

Privacyâ€‘First Browser LLM Fineâ€‘Tuning Studio

PrivyTune lets users fineâ€‘tune small openâ€‘source language models entirely in their browser using WebGPU, with zero serverâ€‘side compute and no raw data ever leaving the device. Users can dragâ€‘andâ€‘drop plaintext or CSV datasets, train a LoRA adapter on a base model (e.g. Phiâ€‘3â€‘mini or GemmaÂ 2B), and export a tiny .safetensors file for use in local runtimes like LMâ€‘Studio, llama.cpp, or WebLLM.

ğŸš€ Key Features

Local Fineâ€‘Tuning: Runs LoRA/QLoRA training loops fully clientâ€‘side via WebGPU compute shaders.

Privacy & Compliance: Raw text and gradients never leave the browser; backend only handles auth and optional adapter sharing.

IndexedDB Caching: Downloads and shards of INT4 models (ONNX/GGUF) are cached for instant reloads and offline use.

Model Marketplace: Backendâ€‘driven manifest API to list, versionâ€‘pin, and licenseâ€‘gate base models (Phiâ€‘3â€‘mini, GemmaÂ 2B).

LoRA Adapter Export: Outputs lowâ€‘rank delta weights (<1â€¯% of model size) as .safetensors for universal compatibility.

Progressive UX: Web Worker tokenization, streaming progress & live loss charts, graceful fallback to WASM/CPU on unsupported devices.

Secure Sharing: Optional presignedâ€‘URL upload of adapters; Spring Security + JWT + Postgres for audit.

ğŸ¯ MVP Scope

Single base model: Xenova/phi-3-mini-4k-instruct-int4

LoRA rankâ€‘16, INT4 quantization

Dataset: CSV/plainâ€‘text with prompt,response columns

Browsers: ChromeÂ 124+, EdgeÂ 124+

Nonâ€‘goals: multiâ€‘adapter stacking, >4â€¯GB VRAM models

ğŸ› ï¸ Tech Stack

Layer

Tech & Tools

Frontend

ReactÂ 18 + TypeScript, Vite, Tailwind CSS, Transformers.js v3 (WebGPU), IDBâ€‘Keyval, Chart.js

Backend

Spring BootÂ 3.4 (JavaÂ 21), Spring Security (JWT), S3 presigned URLs, PostgreSQLÂ 15

DevOps

Dockerâ€‘Compose (local), GitHub Actions CI/CD, Cloudflare Pages (frontend), Railway/Fly.io (backend + DB)

Model I/O

ONNX/GGUF INT4 shards, JSON manifest API, WebGPU execution provider, SentencePiece tokenization (WASM)

ğŸ“¥ Installation & Setup

Clone the repo

git clone https://github.com/MartinKamburov/PrivyTune.git
cd privytune

Environment

Node.js â‰¥â€¯18, npm

JavaÂ 21+, Maven or Gradle

Docker & Dockerâ€‘Compose

Configure

Copy backend/.env.example â†’ .env, set:

DATABASE_URL=postgres://user:pass@localhost:5432/privytune
JWT_SECRET=<yourâ€‘secret>
S3_BUCKET=<yourâ€‘bucket>
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...

Copy frontend/.env.example â†’ .env, set:

VITE_API_BASE_URL=http://localhost:8080/api

Bring up services

docker-compose up -d   # starts Postgres, backend, frontend dev server

Local dev

Backend: cd backend && ./mvnw spring-boot:run

Frontend: cd frontend && npm install && npm run dev

Open http://localhost:5173 in ChromeÂ 124+ or EdgeÂ 124+.

ğŸ“– Usage

Select Base Model â€“ choose from the model list (e.g. Phiâ€‘3â€‘mini).

Import Dataset â€“ drag CSV or paste plaintext (prompt/response pairs).

Fineâ€‘Tune â€“ click â€œTrainâ€; watch live loss curve and progress bar.

Chat & Compare â€“ interact with base & tuned model in sideâ€‘byâ€‘side playground.

Export Adapter â€“ download .safetensors for use in other LLM runtimes.

ğŸ“ Architecture Overview

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Frontend PWA        â”‚â—€â”€â”€â”€â–¶â”‚      Spring Boot Backend     â”‚
â”‚  â€¢ Model loader & cache    â”‚      â”‚  â€¢ Manifest & licensing API  â”‚
â”‚  â€¢ WebGPU training engine  â”‚      â”‚  â€¢ Auth, presigned URL give  â”‚
â”‚  â€¢ Chat + metrics charts   â”‚      â”‚  â€¢ Postgres metadata store   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚     â”‚                               â”‚
            â”‚     â”‚ (WebGPU)                      â”‚ (S3)
            â–¼     â”‚                               â–¼
   GPU Compute    â””â”€â”€ IndexedDB cache  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   (LoRA shaders)                    â”‚   S3 / CDN Storage  â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“„ License

This project is licensed under the MIT License.

ğŸ¤ Contributing

Fork the repo and create a branch (git checkout -b feat/my-feature).

Commit your changes (git commit -m "feat: add ...").

Push to your branch (git push origin feat/my-feature).

Open a Pull Request.

Please ensure all tests pass and linting is green. See CONTRIBUTING.md for guidelines.

ğŸ“¬ Contact

Maintained by martinivkamburov@gmail.com. Pull requests and issues are welcome!